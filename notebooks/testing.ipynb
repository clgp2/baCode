{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82ed27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 10:51:13,783 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-08 10:51:13,787 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-08 10:51:14,987 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-08 10:51:15,051 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-08 10:51:15,120 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-08 10:51:15,145 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-08-08 10:51:29,467 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-08 10:51:30,209 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-08 10:51:30,355 - INFO - joeynmt.prediction - Decoding on dev set (/home/bernadeta/BA_code/data/external_testdata/newsdev2016/tok_newsdev.bpe.ro)...\n",
      "2021-08-08 10:54:24,797 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 10:54:24,797 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 10:54:24,797 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 10:54:25,602 - INFO - joeynmt.prediction -  dev bleu[13a]:  15.82 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 10:54:25,623 - INFO - joeynmt.prediction - Translations saved to: L1_0308_300_ndev..dev\n",
      "2021-08-08 10:54:25,623 - INFO - joeynmt.prediction - Decoding on test set (/home/bernadeta/BA_code/data/external_testdata/newstest2016/newstest_tok_bpe.ro)...\n",
      "2021-08-08 10:57:13,560 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 10:57:13,570 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 10:57:13,570 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 10:57:14,307 - INFO - joeynmt.prediction - test bleu[13a]:  14.92 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 10:57:14,315 - INFO - joeynmt.prediction - Translations saved to: L1_0308_300_ndev..test\n"
     ]
    }
   ],
   "source": [
    "!python3 -m joeynmt test ../JoeyYamlFiles/L1_0308_300_ndev.yaml --output_path L1_0308_300_ndev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee05a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 11:03:29,610 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-08 11:03:29,615 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-08 11:03:30,795 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-08 11:03:30,822 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-08 11:03:30,853 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-08 11:03:30,874 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-08-08 11:08:51,089 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-08 11:08:51,864 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-08 11:08:52,009 - INFO - joeynmt.prediction - Decoding on dev set (/home/bernadeta/BA_code/data/external_testdata/newsdev2016/tok_newsdev.bpe.ro)...\n",
      "2021-08-08 11:11:53,614 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 11:11:53,614 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 11:11:53,614 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 11:11:54,421 - INFO - joeynmt.prediction -  dev bleu[13a]:  20.77 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 11:11:54,434 - INFO - joeynmt.prediction - Translations saved to: L2_0308_300_ndev..dev\n",
      "2021-08-08 11:11:54,434 - INFO - joeynmt.prediction - Decoding on test set (/home/bernadeta/BA_code/data/external_testdata/newstest2016/newstest_tok_bpe.ro)...\n",
      "2021-08-08 11:14:45,652 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 11:14:45,653 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 11:14:45,653 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 11:14:46,391 - INFO - joeynmt.prediction - test bleu[13a]:  19.95 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 11:14:46,398 - INFO - joeynmt.prediction - Translations saved to: L2_0308_300_ndev..test\n"
     ]
    }
   ],
   "source": [
    "!python3 -m joeynmt test ../JoeyYamlFiles/L2_0308_300_ndev.yaml --output_path L2_0308_300_ndev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2055972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-08 10:57:17,005 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-08 10:57:17,009 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-08 10:57:18,191 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-08 10:57:18,221 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-08 10:57:18,251 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-08 10:57:18,274 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-08-08 10:57:26,727 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-08 10:57:28,153 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-08 10:57:28,332 - INFO - joeynmt.prediction - Decoding on dev set (/home/bernadeta/BA_code/data/external_testdata/newsdev2016/tok_newsdev.bpe.ro)...\n",
      "2021-08-08 11:00:33,643 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 11:00:33,654 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 11:00:33,654 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 11:00:34,467 - INFO - joeynmt.prediction -  dev bleu[13a]:  21.02 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 11:00:34,476 - INFO - joeynmt.prediction - Translations saved to: L3_0308_300_ndev..dev\n",
      "2021-08-08 11:00:34,476 - INFO - joeynmt.prediction - Decoding on test set (/home/bernadeta/BA_code/data/external_testdata/newstest2016/newstest_tok_bpe.ro)...\n",
      "2021-08-08 11:03:24,819 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-08 11:03:24,819 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-08 11:03:24,819 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-08 11:03:25,527 - INFO - joeynmt.prediction - test bleu[13a]:  19.99 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-08 11:03:25,534 - INFO - joeynmt.prediction - Translations saved to: L3_0308_300_ndev..test\n"
     ]
    }
   ],
   "source": [
    "!python3 -m joeynmt test ../JoeyYamlFiles/L3_0308_300_ndev.yaml --output_path L3_0308_300_ndev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7051e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "bert-base-multilingual-cased_L9_no-idf_version=0.3.10(hug_trans=4.9.1)_fast-tokenizer P: 0.906524 R: 0.901746 F1: 0.903939\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ../sacrebleuFinal/L2_test.ro -c ../sacrebleuFinal/L3_0308_300.detok.test --lang ro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a05dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-env-python3.8",
   "language": "python",
   "name": "ba-env-python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
