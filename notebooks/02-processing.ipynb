{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, we generate the input data for the neural network\n",
    "* word-level tokenize train, dev and test set with sacremoses\n",
    "* bpe all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word-level tokenize L1, L2 and L3 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_train_en= \"../data/DCEP/01-intermediate/L2_strong/L2_train.en\"\n",
    "L2_train_ro= \"../data/DCEP/01-intermediate/L2_strong/L2_train.ro\"\n",
    "L2_train_en_tok = L2_train_en+\".tok\"\n",
    "L2_train_ro_tok = L2_train_ro+\".tok\"\n",
    "\n",
    "L2_test_en= \"../data/DCEP/01-intermediate/L2_strong/L2_test.en\"\n",
    "L2_test_ro= \"../data/DCEP/01-intermediate/L2_strong/L2_test.ro\"\n",
    "L2_test_en_tok = L2_test_en+\".tok\"\n",
    "L2_test_ro_tok=L2_test_ro+\".tok\"\n",
    "\n",
    "L2_dev_en= \"../data/DCEP/01-intermediate/L2_strong/L2_dev.en\"\n",
    "L2_dev_ro= \"../data/DCEP/01-intermediate/L2_strong/L2_dev.ro\"\n",
    "L2_dev_en_tok = L2_dev_en+\".tok\"\n",
    "L2_dev_ro_tok = L2_dev_ro+\".tok\"\n",
    "\n",
    "! sacremoses -l \"en\" -j 8 tokenize < $L2_train_en > $L2_train_en_tok\n",
    "! sacremoses -l \"ro\" -j 8 tokenize < $L2_train_ro > $L2_train_ro_tok\n",
    "\n",
    "! sacremoses -l \"en\" -j 8 tokenize < $L2_test_en > $L2_test_en_tok\n",
    "! sacremoses -l \"ro\" -j 8 tokenize < $L2_test_ro > $L2_test_ro_tok\n",
    "\n",
    "! sacremoses -l \"en\" -j 8 tokenize < $L2_dev_en > $L2_dev_en_tok\n",
    "! sacremoses -l \"ro\" -j 8 tokenize < $L2_dev_ro > $L2_dev_ro_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_train_en= \"../data/DCEP/01-intermediate/L1_basic/L1_train.en\"\n",
    "L1_train_ro= \"../data/DCEP/01-intermediate/L1_basic/L1_train.ro\"\n",
    "\n",
    "L1_train_en_tok = L1_train_en+\".tok\"\n",
    "L1_train_ro_tok = L1_train_ro+\".tok\"\n",
    "\n",
    "! sacremoses -l \"en\" -j 8 tokenize < $L1_train_en > $L1_train_en_tok\n",
    "! sacremoses -l \"ro\" -j 8 tokenize < $L1_train_ro > $L1_train_ro_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L3_train_en= \"../data/DCEP/01-intermediate/L3_intermediate/L3_train.en\"\n",
    "L3_train_ro= \"../data/DCEP/01-intermediate/L3_intermediate/L3_train.ro\"\n",
    "\n",
    "L3_train_en_tok = L3_train_en+\".tok\"\n",
    "L3_train_ro_tok = L3_train_ro+\".tok\"\n",
    "\n",
    "! sacremoses -l \"en\" -j 8 tokenize < $L3_train_en > $L3_train_en_tok\n",
    "! sacremoses -l \"ro\" -j 8 tokenize < $L3_train_ro > $L3_train_ro_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learn and apply subword tokenization with subword-nmt, an implementation of byte-pair-encoding for subword splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNnmuetQWx4d",
    "outputId": "f11e2776-df3b-43e1-b924-3bcfa4f6b205"
   },
   "outputs": [],
   "source": [
    "#this bpe_size is recommended for small to medium sized datasets (30K-1.3M)\n",
    "bpe_size=8000\n",
    "\n",
    "#learn the vocab from the bigger training files resulted after basic cleaning\n",
    "#the problem with this approach is that the L1 dataset contains wrong languages, which makes it possible to have wrong translations\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input $L1_train_en_tok $L1_train_ro_tok -s $bpe_size -o bpe.codes.$bpe_size --write-vocabulary vocab.en vocab.ro\n",
    "\n",
    "#apply BPE\n",
    "#todo:change path such that the bpe files point to 02-preprocessed!\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L1_train_en_tok > L1_train_tok.bpe.en\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L1_train_ro_tok > L1_train_tok.bpe.ro\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L3_train_en_tok > L3_train_tok.bpe.en\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L3_train_ro_tok > L3_train_tok.bpe.ro\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_train_en_tok > L2_train_tok.bpe.en\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_train_ro_tok > L2_train_tok.bpe.ro\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_dev_en_tok > L2_dev_tok.bpe.en\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_dev_ro_tok > L2_dev_tok.bpe.ro\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_test_en_tok > L2_test_tok.bpe.en\n",
    "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_test_ro_tok > L2_test_tok.bpe.ro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9SWFHa0eZky",
    "outputId": "4cebc0e6-81cc-421d-84ce-d7d35e441e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-19 14:30:50--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
      "Auflösen des Hostnamen »raw.githubusercontent.com (raw.githubusercontent.com)«... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Verbindungsaufbau zu raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... verbunden.\n",
      "HTTP-Anforderung gesendet, warte auf Antwort... 200 OK\n",
      "Länge: 2034 (2,0K) [text/plain]\n",
      "In »»build_vocab.py«« speichern.\n",
      "\n",
      "100%[======================================>] 2.034       --.-K/s   in 0s      \n",
      "\n",
      "2021-08-19 14:30:51 (81,4 MB/s) - »»build_vocab.py«« gespeichert [2034/2034]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
    "\n",
    "! python build_vocab.py L1_train_tok.bpe.en L1_train_tok.bpe.ro --output_path vocab.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: der Aufruf von stat für „bpe.codes“ ist nicht möglich: Datei oder Verzeichnis nicht gefunden\r\n"
     ]
    }
   ],
   "source": [
    "#move all files to the right destination: in DCEP/02-preporcessed/\n",
    "!mv bpe.codes.8000 ../data/DCEP/02-preprocessed/\n",
    "!mv vocab.en ../data/DCEP/02-preprocessed/\n",
    "!mv vocab.ro ../data/DCEP/02-preprocessed/\n",
    "!mv vocab.txt ../data/DCEP/02-preprocessed/\n",
    "!mv build_vocab.py ../data/DCEP/02-preprocessed/\n",
    "\n",
    "!mv L1_train_tok.bpe.en ../data/DCEP/02-preprocessed/L1_basic/\n",
    "!mv L1_train_tok.bpe.ro ../data/DCEP/02-preprocessed/L1_basic/\n",
    "\n",
    "!mv L3_train_tok.bpe.en ../data/DCEP/02-preprocessed/L3_intermediate/\n",
    "!mv L3_train_tok.bpe.ro ../data/DCEP/02-preprocessed/L3_intermediate/\n",
    "\n",
    "!mv L2_train_tok.bpe.en ../data/DCEP/02-preprocessed/L2_strong/\n",
    "!mv L2_train_tok.bpe.ro ../data/DCEP/02-preprocessed/L2_strong/\n",
    "\n",
    "!mv L2_dev_tok.bpe.en ../data/DCEP/02-preprocessed/L2_strong/\n",
    "!mv L2_dev_tok.bpe.ro ../data/DCEP/02-preprocessed/L2_strong/\n",
    "\n",
    "!mv L2_test_tok.bpe.en ../data/DCEP/02-preprocessed/L2_strong/\n",
    "!mv L2_test_tok.bpe.ro ../data/DCEP/02-preprocessed/L2_strong/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "prepare_dcep_new.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a4916cd12ee39cb2350b6989a6656d6368d271ce25ea714d6ddb0152fb90db1c"
  },
  "kernelspec": {
   "display_name": "ba-env-python3.8",
   "language": "python",
   "name": "ba-env-python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
