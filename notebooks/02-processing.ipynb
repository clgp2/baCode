{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_dcep_new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit ('ba-env': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8"
    },
    "interpreter": {
      "hash": "a4916cd12ee39cb2350b6989a6656d6368d271ce25ea714d6ddb0152fb90db1c"
    }
  },
  "cells": [
    {
      "source": [
        "## Here, we generate the input data for the neural network\n",
        "* word-level tokenize train, dev and test set with sacremoses\n",
        "* bpe all files"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_file_train= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_train.txt\")\n",
        "target_file_train= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_train.txt\")\n",
        "tok_source_file_train = source_file_train+\".tok\"\n",
        "tok_target_file_train=target_file_train+\".tok\"\n",
        "\n",
        "source_file_test= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_test.txt\")\n",
        "target_file_test= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_test.txt\")\n",
        "tok_source_file_test = source_file_test+\".tok\"\n",
        "tok_target_file_test=target_file_test+\".tok\"\n",
        "\n",
        "source_file_dev= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_dev.txt\")\n",
        "target_file_dev= os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_dev.txt\")\n",
        "tok_source_file_dev = source_file_dev+\".tok\"\n",
        "tok_target_file_dev=target_file_dev+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_train > $tok_source_file_train\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_train > $tok_target_file_train\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_test > $tok_source_file_test\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_test > $tok_target_file_test\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_dev > $tok_source_file_dev\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_dev > $tok_target_file_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_train_basic= os.path.join(os.pardir,\"/data/02-preprocessed/basic_cleaned/basic_source_train.txt\")\n",
        "target_train_basic= os.path.join(os.pardir,\"/data/02-preprocessed/basic_cleaned/basic_target_train.txt\")\n",
        "\n",
        "tok_source_train_basic = source_train_basic+\".tok\"\n",
        "tok_target_train_basic = target_train_basic+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_train_basic > $tok_source_train_basic\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_train_basic > $tok_target_train_basic"
      ]
    },
    {
      "source": [
        "##  Learn and apply subword tokenization with subword-nmt, an implementation of byte-pair-encoding for subword splitting"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNnmuetQWx4d",
        "outputId": "f11e2776-df3b-43e1-b924-3bcfa4f6b205"
      },
      "source": [
        "#this bpe_size is recommended for small to medium sized datasets (30K-1.3M)\n",
        "bpe_size=8000\n",
        "\n",
        "tok_source_train_basic=os.path.join(os.pardir,\"/data/02-preprocessed/basic_cleaned/basic_source_train.txt.tok\")\n",
        "tok_target_train_basic=os.path.join(os.pardir,\"/data/02-preprocessed/basic_cleaned/basic_target_train.txt.tok\")\n",
        "\n",
        "tok_source_train_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_train.txt.tok\")\n",
        "tok_target_train_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_train.txt.tok\")\n",
        "\n",
        "tok_source_dev_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_dev.txt.tok\")\n",
        "tok_target_dev_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_dev.txt.tok\")\n",
        "\n",
        "tok_source_test_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_test.txt.tok\")\n",
        "tok_target_test_bicleaner=os.path.join(os.pardir,\"/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_test.txt.tok\")\n",
        "\n",
        "#learn the vocab from the bigger training files resulted after basic cleaning\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input $tok_source_train_basic $tok_target_train_basic -s $bpe_size -o bpe.codes.$bpe_size --write-vocabulary vocab.en vocab.ro\n",
        "\n",
        "#apply BPE\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_train_basic > tok_train_basic.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_train_basic > tok_train_basic.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_train_bicleaner > tok_train_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_train_bicleaner > tok_train_bicleaner.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_dev_bicleaner > tok_dev_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_dev_bicleaner > tok_dev_bicleaner.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_test_bicleaner > tok_test_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_test_bicleaner > tok_test_bicleaner.bpe.ro"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9SWFHa0eZky",
        "outputId": "4cebc0e6-81cc-421d-84ce-d7d35e441e53"
      },
      "source": [
        "#! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
        "\n",
        "! python build_vocab.py tok_source_train_basic.bpe.en tok_source_train_basic.bpe.ro --output_path vocab.txt\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
        "mt = MosesTokenizer(lang='en')\n",
        "text = u'In order to comply with the obligation to provide effective non-discriminatory access to existing infrastructures, policies or procedures within the meaning of A rticle 4 (1) (a), Member States may, if necessary, have an additional period of 10 years [from the deadline for transposition] to comply with that obligation                                                      .'\n",
        "\n",
        "tokenized_text = mt.tokenize(text, return_str=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed writing EN_tok.txt\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer\n",
        "from subword_nmt import apply_bpe\n",
        "\n",
        "mt_en = MosesTokenizer(lang='en')\n",
        "\n",
        "source_input_file = \"/home/bernadeta/BA_code/wmt16-en-ro.src\"\n",
        "\n",
        "with open(source_input_file) as rawfile, open(\"wmt16-en.tok\", \"w\") as tokfile:\n",
        "     for i, line in enumerate(rawfile):\n",
        "         data=line.rstrip()\n",
        "         tokfile.write(mt_en.tokenize(data, return_str=True)+\"\\n\")\n",
        "\n",
        "print(\"Completed writing EN_tok.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe_file=\"/home/bernadeta/BA_code/data/02-preprocessed/bpe.codes.8000\"\n",
        "\n",
        "with open(bpe_file, \"r\") as merge_file:\n",
        "    bpe=apply_bpe.BPE(codes=merge_file)\n",
        "\n",
        "with open(\"wmt16-en.tok\") as src_tok_file, open(\"wmt16.8000.bpe.en\", \"w\") as src_bpefile:\n",
        "    for i, line in enumerate(src_tok_file):\n",
        "        data=line.strip()\n",
        "        bpedata=bpe.process_line(data)\n",
        "        src_bpefile.write(bpedata+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}