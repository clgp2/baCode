{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_dcep_new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python368jvsc74a57bd03a49f9c478bb8acf79e8d4394dd772c4b6effd3dc1bcc10da60d89375c51c795",
      "display_name": "Python 3.6.8 64-bit ('ba-env': venv)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8"
    },
    "interpreter": {
      "hash": "3a49f9c478bb8acf79e8d4394dd772c4b6effd3dc1bcc10da60d89375c51c795"
    }
  },
  "cells": [
    {
      "source": [
        "## Here, we generate the input data for the neural network\n",
        "* word-level tokenize train, dev set with sacremoses\n",
        "* bpe all files"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_file_train= \"bicleaner_source_train.txt\"\n",
        "target_file_train= \"bicleaner_target_train.txt\"\n",
        "tok_source_file_train = source_file_train+\".tok\"\n",
        "tok_target_file_train=target_file_train+\".tok\"\n",
        "\n",
        "source_file_test= \"bicleaner_source_test.txt\"\n",
        "target_file_test= \"bicleaner_target_test.txt\"\n",
        "tok_source_file_test = source_file_test+\".tok\"\n",
        "tok_target_file_test=target_file_test+\".tok\"\n",
        "\n",
        "source_file_dev= \"bicleaner_source_dev.txt\"\n",
        "target_file_dev= \"bicleaner_target_dev.txt\"\n",
        "tok_source_file_dev = source_file_dev+\".tok\"\n",
        "tok_target_file_dev=target_file_dev+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_train > $tok_source_file_train\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_train > $tok_target_file_train\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_test > $tok_source_file_test\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_test > $tok_target_file_test\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_file_dev > $tok_source_file_dev\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_file_dev > $tok_target_file_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_train_basic= \"basic_source_train.txt\"\n",
        "target_train_basic= \"basic_target_train.txt\"\n",
        "\n",
        "tok_source_train_basic = source_train_basic+\".tok\"\n",
        "tok_target_train_basic = target_train_basic+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $source_train_basic > $tok_source_train_basic\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $target_train_basic > $tok_target_train_basic"
      ]
    },
    {
      "source": [
        "##  Learn and apply subword tokenization with subword-nmt, an implementation of byte-pair-encoding for subword splitting"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNnmuetQWx4d",
        "outputId": "f11e2776-df3b-43e1-b924-3bcfa4f6b205"
      },
      "source": [
        "#this bpe_size is recommended for small to medium sized datasets (30K-1.3M)\n",
        "bpe_size=8000\n",
        "\n",
        "tok_source_train_basic=\"/home/bernadeta/BA_code/data/02-preprocessed/basic_cleaned/basic_source_train.txt.tok\"\n",
        "tok_target_train_basic=\"/home/bernadeta/BA_code/data/02-preprocessed/basic_cleaned/basic_target_train.txt.tok\"\n",
        "\n",
        "tok_source_train_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_train.txt.tok\"\n",
        "tok_target_train_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_train.txt.tok\"\n",
        "\n",
        "tok_source_dev_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_dev.txt.tok\"\n",
        "tok_target_dev_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_dev.txt.tok\"\n",
        "\n",
        "tok_source_test_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_source_test.txt.tok\"\n",
        "tok_target_test_bicleaner=\"/home/bernadeta/BA_code/data/02-preprocessed/bicleaner_cleaned/bicleaner_target_test.txt.tok\"\n",
        "\n",
        "#learn the vocab from the bigger training files resulted after basic cleaning\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input $tok_source_train_basic $tok_target_train_basic -s $bpe_size -o bpe.codes.$bpe_size --write-vocabulary vocab.en vocab.ro\n",
        "\n",
        "#apply BPE\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_train_basic > tok_source_train_basic.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_train_basic > tok_source_train_basic.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_train_bicleaner > tok_source_train_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_train_bicleaner > tok_source_train_bicleaner.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_dev_bicleaner > tok_source_dev_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_dev_bicleaner > tok_target_dev_bicleaner.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $tok_source_test_bicleaner > tok_source_test_bicleaner.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $tok_target_test_bicleaner > tok_target_test_bicleaner.bpe.ro"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9SWFHa0eZky",
        "outputId": "4cebc0e6-81cc-421d-84ce-d7d35e441e53"
      },
      "source": [
        "#! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
        "\n",
        "! python build_vocab.py tok_source_train_basic.bpe.en tok_source_train_basic.bpe.ro --output_path vocab.txt\n"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}