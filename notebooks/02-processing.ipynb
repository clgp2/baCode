{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Here, we generate the input data for the neural network\n",
        "* word-level tokenize train, dev and test set with sacremoses\n",
        "* bpe all files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### word-level tokenize L1, L2 and L3 splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L2_train_en= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_train.en\")\n",
        "L2_train_ro= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_train.ro\")\n",
        "L2_train_en_tok = L2_train_en+\".tok\"\n",
        "L2_train_ro_tok = L2_train_ro+\".tok\"\n",
        "\n",
        "L2_test_en= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_test.en\")\n",
        "L2_test_ro= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_test.ro\")\n",
        "L2_test_en_tok = L2_test_en+\".tok\"\n",
        "L2_test_ro_tok=L2_test_ro+\".tok\"\n",
        "\n",
        "L2_dev_en= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_dev.en\")\n",
        "L2_dev_ro= os.path.join(os.pardir,\"/data/DCEP/01-intermediate/L2_strong/L2_dev.ro\")\n",
        "L2_dev_en_tok = L2_dev_en+\".tok\"\n",
        "L2_dev_ro_tok = L2_dev_ro+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $L2_train_en > $L2_train_en_tok\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $L2_train_ro > $L2_train_ro_tok\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $L2_test_en > $L2_test_en_tok\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $L2_test_ro > $L2_test_ro_tok\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $L2_dev_en > $L2_dev_en_tok\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $L2_dev_ro > $L2_dev_ro_tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_train_en= os.path.join(os.pardir,\"data/DCEP/01-intermediate/L1_basic/L1_train.en\")\n",
        "L1_train_ro= os.path.join(os.pardir,\"data/DCEP/01-intermediate/L1_basic/L1_train.ro\")\n",
        "\n",
        "L1_train_en_tok = L1_train_en+\".tok\"\n",
        "L1_train_ro_tok = L1_train_ro+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $L1_train_en > $L1_train_en_tok\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $L1_train_ro > $L1_train_ro_tok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L3_train_en= os.path.join(os.pardir,\"data/DCEP/01-intermediate/L3_intermediate/L3_train.en\")\n",
        "L3_train_ro= os.path.join(os.pardir,\"data/DCEP/01-intermediate/L3_intermediate/L3_train.ro\")\n",
        "\n",
        "L3_train_en_tok = L3_train_en+\".tok\"\n",
        "L3_train_ro_tok = L3_train_ro+\".tok\"\n",
        "\n",
        "! sacremoses -l \"en\" -j 8 tokenize < $L3_train_en > $L3_train_en_tok\n",
        "! sacremoses -l \"ro\" -j 8 tokenize < $L3_train_ro > $L3_train_ro_tok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Learn and apply subword tokenization with subword-nmt, an implementation of byte-pair-encoding for subword splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNnmuetQWx4d",
        "outputId": "f11e2776-df3b-43e1-b924-3bcfa4f6b205"
      },
      "outputs": [],
      "source": [
        "#this bpe_size is recommended for small to medium sized datasets (30K-1.3M)\n",
        "bpe_size=8000\n",
        "\n",
        "#learn the vocab from the bigger training files resulted after basic cleaning\n",
        "#the problem with this approach is that the L1 dataset contains wrong languages, which makes it possible to have wrong translations\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input $L1_train_en_tok $L1_train_ro_tok -s $bpe_size -o bpe.codes.$bpe_size --write-vocabulary vocab.en vocab.ro\n",
        "\n",
        "#apply BPE\n",
        "#todo:change path such that the bpe files point to 02-preprocessed!\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L1_train_en_tok > L1_train_tok.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L1_train_ro_tok > L1_train_tok.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L3_train_en_tok > L3_train_tok.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L3_train_ro_tok > L3_train_tok.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_train_en_tok > L2_train_tok.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_train_ro_tok > L2_train_tok.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_dev_en_tok > L2_dev_tok.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_dev_ro_tok > L2_dev_tok.bpe.ro\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.en --vocabulary-threshold 50 < $L2_test_en_tok > L2_test_tok.bpe.en\n",
        "! subword-nmt apply-bpe -c bpe.codes.$bpe_size --vocabulary vocab.ro --vocabulary-threshold 50 < $L2_test_en_tok > L2_test_tok.bpe.ro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9SWFHa0eZky",
        "outputId": "4cebc0e6-81cc-421d-84ce-d7d35e441e53"
      },
      "outputs": [],
      "source": [
        "#! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
        "\n",
        "! python build_vocab.py L1_train_tok.bpe.en L1_train_tok.bpe.ro --output_path vocab.txt\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "prepare_dcep_new.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a4916cd12ee39cb2350b6989a6656d6368d271ce25ea714d6ddb0152fb90db1c"
    },
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit ('ba-env': venv)",
      "language": "python",
      "name": "python368jvsc74a57bd03a49f9c478bb8acf79e8d4394dd772c4b6effd3dc1bcc10da60d89375c51c795"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}